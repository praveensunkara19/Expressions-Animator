{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f970ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ca8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4692b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For static images:\n",
    "\n",
    "# Read all images from folder\n",
    "image_dir = \"C:\\\\Users\\\\prave\\\\OneDrive\\\\Desktop\\\\Git\\\\Face-Animator\\\\test\"\n",
    "IMAGE_FILES = [os.path.join(image_dir, file)\n",
    "               for file in os.listdir(image_dir)\n",
    "               if file.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5) as face_mesh:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    image = cv2.imread(file)\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Print and draw face mesh landmarks on the image.\n",
    "    if not results.multi_face_landmarks:\n",
    "      continue\n",
    "    annotated_image = image.copy()\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "      print('face_landmarks:', face_landmarks)\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=annotated_image,\n",
    "          landmark_list=face_landmarks,\n",
    "          connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp_drawing_styles\n",
    "          .get_default_face_mesh_tesselation_style())\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=annotated_image,\n",
    "          landmark_list=face_landmarks,\n",
    "          connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp_drawing_styles\n",
    "          .get_default_face_mesh_contours_style())\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=annotated_image,\n",
    "          landmark_list=face_landmarks,\n",
    "          connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp_drawing_styles\n",
    "          .get_default_face_mesh_iris_connections_style())\n",
    "    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f33e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffpyplayer.player import MediaPlayer\n",
    "import cv2\n",
    "\n",
    "def PlayVideo(video_path):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    player = MediaPlayer(video_path)\n",
    "\n",
    "    while True:\n",
    "        grabbed, frame = video.read()\n",
    "        audio_frame, val = player.get_frame()\n",
    "\n",
    "        if not grabbed:\n",
    "            print(\"End of video\")\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"Video\", frame)\n",
    "\n",
    "        if val != 'eof' and audio_frame is not None:\n",
    "            img, t = audio_frame\n",
    "\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "PlayVideo(\"C:\\\\Users\\\\prave\\\\OneDrive\\\\Desktop\\\\Git\\\\Face-Animator\\\\test\\\\exp_test.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a235a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ffpyplayer.player import MediaPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169338bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Drawing specifications\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "# Video path\n",
    "video_path = \"C:\\\\Users\\\\prave\\\\OneDrive\\\\Desktop\\\\Git\\\\Face-Animator\\\\test\\\\exp_test.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "player = MediaPlayer(video_path)\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=3,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.8,\n",
    "        min_tracking_confidence=0.8) as face_mesh:\n",
    "\n",
    "        while cap.isOpened():\n",
    "            start_time = time.time()  # Start inference timer\n",
    "\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Video stream ended.\")\n",
    "                break\n",
    "\n",
    "            image.flags.writeable = False\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = face_mesh.process(image_rgb)\n",
    "\n",
    "            # Retrieve and play audio frame\n",
    "            audio_frame, val = player.get_frame()\n",
    "            if val == 'eof':\n",
    "                print(\"Audio stream ended.\")\n",
    "                break\n",
    "\n",
    "            image.flags.writeable = True\n",
    "\n",
    "            # Draw face landmarks\n",
    "            if results.multi_face_landmarks:\n",
    "                for face_landmarks in results.multi_face_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=image,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=image,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=image,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "            # Calculate and display inference time\n",
    "            end_time = time.time()\n",
    "            inference_time_ms = (end_time - start_time) * 1000\n",
    "            cv2.putText(image, f\"Inference time: {inference_time_ms:.2f} ms\",\n",
    "                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            # Display the output frame\n",
    "            cv2.imshow('MediaPipe Face Mesh', image)\n",
    "\n",
    "            # Exit on pressing 'q'\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                print(\"Exiting on user command.\")\n",
    "                break\n",
    "\n",
    "finally:\n",
    "    # Always release resources and close windows\n",
    "    cap.release()\n",
    "    player.close_player()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Resources released. Program ended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addf1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f8bed",
   "metadata": {},
   "source": [
    "Face alignment using the keypoints of the eyes,lips,and nose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6980ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Drawing specifications\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "# Video path\n",
    "video_path = \"C:\\\\Users\\\\prave\\\\OneDrive\\\\Desktop\\\\Git\\\\Face-Animator\\\\test\\\\exp_test.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Standard face landmark indices for alignment\n",
    "LEFT_EYE_IDX = 33   # Approximate landmark index for left eye\n",
    "RIGHT_EYE_IDX = 263 # Approximate landmark index for right eye\n",
    "NOSE_IDX = 1        # Nose tip\n",
    "\n",
    "# Desired output image size and landmark positions (canonical template)\n",
    "output_size = (512, 512)\n",
    "# Slight zoom out by reducing eye distance\n",
    "dst_pts = np.float32([\n",
    "    [230, 240],  # Left eye moved further right\n",
    "    [282, 240],  # Right eye moved further left\n",
    "    [256, 340]   # Nose tip moved slightly higher\n",
    "])\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.8,\n",
    "        min_tracking_confidence=0.8) as face_mesh:\n",
    "\n",
    "        while cap.isOpened():\n",
    "            start_time = time.time()\n",
    "\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Video stream ended.\")\n",
    "                break\n",
    "\n",
    "            image.flags.writeable = False\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = face_mesh.process(image_rgb)\n",
    "            image.flags.writeable = True\n",
    "\n",
    "            if results.multi_face_landmarks:\n",
    "                for face_landmarks in results.multi_face_landmarks:\n",
    "                    # Extract landmark coordinates for alignment\n",
    "                    h, w, _ = image.shape\n",
    "                    left_eye = face_landmarks.landmark[LEFT_EYE_IDX]\n",
    "                    right_eye = face_landmarks.landmark[RIGHT_EYE_IDX]\n",
    "                    nose = face_landmarks.landmark[NOSE_IDX]\n",
    "\n",
    "                    src_pts = np.float32([\n",
    "                        [left_eye.x * w, left_eye.y * h],\n",
    "                        [right_eye.x * w, right_eye.y * h],\n",
    "                        [nose.x * w, nose.y * h]\n",
    "                    ])\n",
    "\n",
    "                    # Compute affine transform for alignment\n",
    "                    M = cv2.getAffineTransform(src_pts, dst_pts)\n",
    "                    aligned_image = cv2.warpAffine(image, M, output_size)\n",
    "\n",
    "                    # Draw landmarks on aligned image\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=aligned_image,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=aligned_image,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=aligned_image,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "                    # Calculate and display inference time\n",
    "                    end_time = time.time()\n",
    "                    inference_time_ms = (end_time - start_time) * 1000\n",
    "                    cv2.putText(aligned_image, f\"Inference time: {inference_time_ms:.2f} ms\",\n",
    "                                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "                    # Display aligned output\n",
    "                    cv2.imshow('Aligned Face Mesh', aligned_image)\n",
    "\n",
    "            # Exit on pressing 'q'\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                print(\"Exiting on user command.\")\n",
    "                break\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Resources released. Program ended.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c3f64",
   "metadata": {},
   "source": [
    "Face Alignment ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe0eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing specifications\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "# Video path\n",
    "video_path = \"C:\\\\Users\\\\prave\\\\OneDrive\\\\Desktop\\\\Git\\\\Face-Animator\\\\test\\\\exp_test.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Standard face landmark indices for alignment\n",
    "LEFT_EYE_IDX = 33   # Approximate left eye\n",
    "RIGHT_EYE_IDX = 263 # Approximate right eye\n",
    "NOSE_IDX = 1        # Nose tip\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.8,\n",
    "        min_tracking_confidence=0.8) as face_mesh:\n",
    "\n",
    "        while cap.isOpened():\n",
    "            start_time = time.time()\n",
    "\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Video stream ended.\")\n",
    "                break\n",
    "\n",
    "            image.flags.writeable = False\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = face_mesh.process(image_rgb)\n",
    "            image.flags.writeable = True\n",
    "\n",
    "            if results.multi_face_landmarks:\n",
    "                for face_landmarks in results.multi_face_landmarks:\n",
    "                    # Extract original frame size\n",
    "                    h, w, _ = image.shape\n",
    "\n",
    "                    # Scaling factor (e.g. 1.5x larger)\n",
    "                    scale = 1.5\n",
    "                    output_width = int(w * scale)\n",
    "                    output_height = int(h * scale)\n",
    "                    output_size = (output_width, output_height)\n",
    "\n",
    "                    # Destination landmark positions relative to output size\n",
    "                    dst_pts = np.float32([\n",
    "                        [output_width * 0.35, output_height * 0.4],  # Left eye\n",
    "                        [output_width * 0.65, output_height * 0.4],  # Right eye\n",
    "                        [output_width * 0.5, output_height * 0.6]    # Nose tip\n",
    "                    ])\n",
    "\n",
    "                    # Extract landmark coordinates for alignment\n",
    "                    left_eye = face_landmarks.landmark[LEFT_EYE_IDX]\n",
    "                    right_eye = face_landmarks.landmark[RIGHT_EYE_IDX]\n",
    "                    nose = face_landmarks.landmark[NOSE_IDX]\n",
    "\n",
    "                    src_pts = np.float32([\n",
    "                        [left_eye.x * w, left_eye.y * h],\n",
    "                        [right_eye.x * w, right_eye.y * h],\n",
    "                        [nose.x * w, nose.y * h]\n",
    "                    ])\n",
    "\n",
    "                    # Compute affine transform\n",
    "                    M = cv2.getAffineTransform(src_pts, dst_pts)\n",
    "                    aligned_image = cv2.warpAffine(image, M, output_size)\n",
    "\n",
    "                    # Draw landmarks on aligned image\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=aligned_image,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=aligned_image,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=aligned_image,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "                    # Calculate and display inference time\n",
    "                    end_time = time.time()\n",
    "                    inference_time_ms = (end_time - start_time) * 1000\n",
    "                    cv2.putText(aligned_image, f\"Inference time: {inference_time_ms:.2f} ms\",\n",
    "                                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "                    # Display aligned output\n",
    "                    cv2.imshow('Aligned Face Mesh', aligned_image)\n",
    "\n",
    "            # Exit on pressing 'q'\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                print(\"Exiting on user command.\")\n",
    "                break\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Resources released. Program ended.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
